{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfdb3d7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9389/2394049005.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Название среды\n",
    "CONST_ENV_NAME = 'Taxi-v3'\n",
    "# Использование GPU\n",
    "CONST_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Элемент ReplayMemory в форме именованного кортежа\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# Реализация техники Replay Memory\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        '''\n",
    "        Сохранение данных в ReplayMemory\n",
    "        '''\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Выборка случайных элементов размера batch_size\n",
    "        '''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN_Model(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        '''\n",
    "        Инициализация топологии нейронной сети\n",
    "        '''\n",
    "        super(DQN_Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Прямой проход\n",
    "        Вызывается для одного элемента, чтобы определить следующее действие\n",
    "        Или для batch'а во время процедуры оптимизации \n",
    "        '''\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "\n",
    "class DQN_Agent:\n",
    "\n",
    "    def __init__(self, env, \n",
    "                 BATCH_SIZE = 128, \n",
    "                 GAMMA = 0.99, \n",
    "                 EPS_START = 0.9, \n",
    "                 EPS_END = 0.05, \n",
    "                 EPS_DECAY = 1000, \n",
    "                 TAU = 0.005, \n",
    "                 LR = 1e-4\n",
    "                 ):\n",
    "        # Среда\n",
    "        self.env = env\n",
    "        # Размерности Q-модели\n",
    "        self.n_actions = env.action_space.n\n",
    "        state, _ = self.env.reset()\n",
    "        self.n_observations = len(state)\n",
    "        # Коэффициенты\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.GAMMA = GAMMA\n",
    "        self.EPS_START = EPS_START\n",
    "        self.EPS_END = EPS_END\n",
    "        self.EPS_DECAY = EPS_DECAY\n",
    "        self.TAU = TAU\n",
    "        self.LR = LR\n",
    "        # Модели\n",
    "        # Основная модель\n",
    "        self.policy_net = DQN_Model(self.n_observations, self.n_actions).to(CONST_DEVICE)\n",
    "        # Вспомогательная модель, используется для стабилизации алгоритма\n",
    "        # Обновление контролируется гиперпараметром TAU\n",
    "        # Используется подход Double DQN\n",
    "        self.target_net = DQN_Model(self.n_observations, self.n_actions).to(CONST_DEVICE)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        # Оптимизатор\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.LR, amsgrad=True)\n",
    "        # Replay Memory\n",
    "        self.memory = ReplayMemory(10000)\n",
    "        # Количество шагов\n",
    "        self.steps_done = 0\n",
    "        # Длительность эпизодов\n",
    "        self.episode_durations = []\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        '''\n",
    "        Выбор действия\n",
    "        '''\n",
    "        sample = random.random()\n",
    "        eps = self.EPS_END + (self.EPS_START - self.EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done / self.EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps:\n",
    "            with torch.no_grad():\n",
    "                # Если вероятность больше eps\n",
    "                # то выбирается действие, соответствующее максимальному Q-значению\n",
    "                # t.max(1) возвращает максимальное значение колонки для каждой строки\n",
    "                # [1] возвращает индекс максимального элемента\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            # Если вероятность меньше eps\n",
    "            # то выбирается случайное действие\n",
    "            return torch.tensor([[self.env.action_space.sample()]], device=CONST_DEVICE, dtype=torch.long)\n",
    "\n",
    "\n",
    "    def plot_durations(self, show_result=False):\n",
    "        plt.figure(1)\n",
    "        durations_t = torch.tensor(self.episode_durations, dtype=torch.float)\n",
    "        if show_result:\n",
    "            plt.title('Результат')\n",
    "        else:\n",
    "            plt.clf()\n",
    "            plt.title('Обучение...')\n",
    "        plt.xlabel('Эпизод')\n",
    "        plt.ylabel('Количество шагов в эпизоде')\n",
    "        plt.plot(durations_t.numpy())\n",
    "        plt.pause(0.001)  # пауза\n",
    "\n",
    "\n",
    "    def optimize_model(self):\n",
    "        '''\n",
    "        Оптимизация модели\n",
    "        '''\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "        # Транспонирование batch'а \n",
    "        # (https://stackoverflow.com/a/19343/3343043)\n",
    "        # Конвертация batch-массива из Transition\n",
    "        # в Transition batch-массивов.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Вычисление маски нефинальных состояний и конкатенация элементов batch'а\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=CONST_DEVICE, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Вычисление Q(s_t, a)\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Вычисление V(s_{t+1}) для всех следующих состояний\n",
    "        next_state_values = torch.zeros(self.BATCH_SIZE, device=CONST_DEVICE)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
    "        # Вычисление ожидаемых значений Q\n",
    "        expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "\n",
    "        # Вычисление Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Оптимизация модели\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def play_agent(self):\n",
    "        '''\n",
    "        Проигрывание сессии для обученного агента\n",
    "        '''\n",
    "        env2 = gym.make(CONST_ENV_NAME, render_mode='human')\n",
    "        state = env2.reset()[0]\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=CONST_DEVICE).unsqueeze(0)\n",
    "        done = False\n",
    "        res = []\n",
    "        while not done:\n",
    "\n",
    "            action = self.select_action(state)\n",
    "            action = action.item()\n",
    "            observation, reward, terminated, truncated, _ = env2.step(action)\n",
    "            env2.render()\n",
    "\n",
    "            res.append((action, reward))\n",
    "\n",
    "            if terminated:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(observation, dtype=torch.float32, device=CONST_DEVICE).unsqueeze(0)\n",
    "\n",
    "            state = next_state\n",
    "            if terminated or truncated:\n",
    "                done = True\n",
    "        \n",
    "        print('Данные об эпизоде: ', res)\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        '''\n",
    "        Обучение агента\n",
    "        '''\n",
    "        if torch.cuda.is_available():\n",
    "            num_episodes = 600\n",
    "        else:\n",
    "            num_episodes = 50\n",
    "\n",
    "        for i_episode in range(num_episodes):\n",
    "            # Инициализация среды\n",
    "            state, info = self.env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=CONST_DEVICE).unsqueeze(0)\n",
    "            for t in count():\n",
    "                action = self.select_action(state)\n",
    "                observation, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "                reward = torch.tensor([reward], device=CONST_DEVICE)\n",
    "                \n",
    "                done = terminated or truncated\n",
    "                if terminated:\n",
    "                    next_state = None\n",
    "                else:\n",
    "                    next_state = torch.tensor(observation, dtype=torch.float32, device=CONST_DEVICE).unsqueeze(0)\n",
    "\n",
    "                # Сохранение данных в Replay Memory\n",
    "                self.memory.push(state, action, next_state, reward)\n",
    "\n",
    "                # Переход к следующему состоянию\n",
    "                state = next_state\n",
    "\n",
    "                # Выполнение одного шага оптимизации модели\n",
    "                self.optimize_model()\n",
    "\n",
    "                # Обновление весов target-сети \n",
    "                # θ′ ← τ θ + (1 − τ )θ′\n",
    "                target_net_state_dict = self.target_net.state_dict()\n",
    "                policy_net_state_dict = self.policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*self.TAU + target_net_state_dict[key]*(1-self.TAU)\n",
    "                self.target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "                if done:\n",
    "                    self.episode_durations.append(t + 1)\n",
    "                    self.plot_durations()\n",
    "                    break\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = gym.make(CONST_ENV_NAME)\n",
    "    agent = DQN_Agent(env)\n",
    "    agent.learn()\n",
    "    agent.play_agent()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
